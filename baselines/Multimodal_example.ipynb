{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Noik9q9c7Bhm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# [Task 2C: Multimodal Propagandistic Memes Classification](https://araieval.gitlab.io/task2/) at [ArabicNLP 2024](https://arabicnlp2024.sigarab.org/) @ACL 2024\n",
    "\n",
    "@Author: Md. Arid Hasan\n",
    "\n",
    "Given multimodal content (text extracted from meme and the meme itself) the task is to detect whether the content is propagandistic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYZ96DWt-TZk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### installing required libraries.\n",
    " - transformers\n",
    " - datasets\n",
    " - evaluate\n",
    " - accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SLJh5GGU-xET",
    "outputId": "b6520066-6946-4030-d523-892082c132d1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install --upgrade accelerate\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-_w4YehCgX4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting up the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lemacEHujmj4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate=2e-5\n",
    "num_train_epochs=2\n",
    "train_max_seq_len = 512\n",
    "max_train_samples = None\n",
    "max_eval_samples=None\n",
    "max_predict_samples=None\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhVWUJ3A_hx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define custom dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VIUAU0rRBOmR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, ids, text_data, image_data, labels, is_test=False):\n",
    "        self.text_data = text_data\n",
    "        self.image_data = image_data\n",
    "        self.ids = ids\n",
    "        self.is_test = is_test\n",
    "        #if not self.is_test:\n",
    "        self.labels = labels\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased') #bert-base-multilingual-uncased\n",
    "        self.transform = transforms.Compose([transforms.Resize(256),\n",
    "                                             transforms.CenterCrop(224),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                             ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        text = self.text_data[index]\n",
    "        image = self.image_data[index]\n",
    "        #if not self.is_test:\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # tokenize text data\n",
    "        text = self.tokenizer.encode_plus(text, add_special_tokens=True,\n",
    "                                           max_length=train_max_seq_len, padding='max_length',\n",
    "                                           return_attention_mask=True, return_tensors='pt')\n",
    "\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        image = self.transform(Image.open(image).convert(\"RGB\"))\n",
    "\n",
    "        fdata = {\n",
    "            'id': id,\n",
    "            'text': text['input_ids'].squeeze(0),\n",
    "            'text_mask': text['attention_mask'].squeeze(0),\n",
    "            'img_path': image,\n",
    "        }\n",
    "        if not self.is_test:\n",
    "            fdata['label'] = torch.tensor(label, dtype=torch.long)\n",
    "            return fdata\n",
    "        else:\n",
    "            return fdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP6CdL7NHpxJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download data from HF: https://huggingface.co/datasets/QCRI/Prop2Hate-Meme\n",
    "### Defining the training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "5d1f1ce8b5df4419a18c8ad648998064",
      "22d0c9f4eb0d4f42a74764b3dfb20545",
      "bc21e2a523214725a2691405a3b1fbbd",
      "8bb6eb7dfc964182a65a76e1378710e7",
      "5be9de19b5ac4925bbb0d0985332a5f8",
      "0420dad7752f4a1c95fd90cbaf23f9bd",
      "8147eb3b9f244fbd9608105325b5e35f",
      "29c9bd1943d54c91af9ab3609d1fc416",
      "409a40649f384a6ea402b7fedd262cd0",
      "c8d25da0b3d04504b5e85a5fb6695c10",
      "b049099cfa0247b1979b7e91edea3eb3",
      "47d84c7c2080480a8536b3b66904b191",
      "a4cb6928d3394b01874d2f607b68dc01",
      "e778e84a6f87474c85900e3b4602837e",
      "7b221543eecb4050983941ef833ffa7b",
      "5ade255d0f6b44fa852f5a87a5b17947",
      "545c5c66fcfa43a5b93fb2320cea6550",
      "3138d0180f454a1e97674d29628c9024",
      "fcca176a6f21409d963b8863b2fd2da1",
      "c62db90501354f7fa0df1d0ad538d0cd",
      "39cd8f7719324047868db4b4c429390d",
      "e3c923e3ec46446bbde0973a62c11819",
      "9a63e319aec644daadf77ac169acfe4d",
      "8ab5c07ef99d4987a24f40da6daa32ac",
      "c4cc04a0f5d342309fa9abc2e063d0bb",
      "310dd31d4197455c8ff494db8f37d03a",
      "511d7e40069b4adfacde110b7efcc65b",
      "429557382fab4cfbbfaa9ef3c5d9a479",
      "bfb6ae0ab8c14854a54f14c9a9302f09",
      "7e4edda7e344435793d48989a4003c70",
      "7f0dd68ea8d447e4b4d62aff724de9e0",
      "954bfb431f0c43beac2918bbbeb111fe",
      "c00312da61464a7ab31cfa3d2746df80"
     ]
    },
    "id": "KBe3fs9uarvU",
    "outputId": "54c08f6c-f7af-41dc-9959-6b0dde2a0385"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cbef1d642a440bb5a1b93db35201cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5d5c4018554fe1a40a0c31b78b1fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc0373a2dd3427d96616889ca958186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"QCRI/Prop2Hate-Meme\")\n",
    "\n",
    "# Specify the directory where you want to save the dataset\n",
    "\n",
    "output_dir=\"./Prop2Hate-Meme\"\n",
    "\n",
    "# Save the dataset to the specified directory. This will save all splits to the output directory.\n",
    "dataset.save_to_disk(output_dir)\n",
    "\n",
    "# If you want to get the raw images from HF dataset format\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Directory to save the images\n",
    "output_dir=\"./Prop2Hate-Meme/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over the dataset and save each image\n",
    "for split in ['train','dev','test']:\n",
    "    jsonl_path = os.path.join(output_dir, f\"arabic_hateful_meme_{split}.jsonl\")\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for idx, item in enumerate(dataset[split]):\n",
    "            # Access the image directly as it's already a PIL.Image object\n",
    "            image = item['image']\n",
    "            image_path = os.path.join(output_dir, item['img_path'])\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
    "            image.save(image_path)\n",
    "            del item['image']\n",
    "            del item['prop_label']\n",
    "            del item['hate_fine_grained_label']\n",
    "            item['label'] = item.pop('hate_label')\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bMzfE34iHyGV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"Prop2Hate-Meme\")\n",
    "\n",
    "train_file = './arabic_hateful_meme_train.jsonl'\n",
    "validation_file = './arabic_hateful_meme_dev.jsonl'\n",
    "test_file = './arabic_hateful_meme_test.jsonl'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAV7v54pdm-T",
    "outputId": "2eb50484-6c38-42ae-f541-75c7b362ee47"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqCBQCIWdykt",
    "outputId": "0aab59ab-a9e4-48f5-8e6b-db45716540c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2143 entries from ./arabic_hateful_meme_train.jsonl\n",
      "First entry:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_images_part2/www.pinterest.com_pin_374924737743995066/7485ad3c9c4cd8159ce93997a18a53a8.jpg',\n",
       " 'text': 'Ø²ÙˆØ¬Ø© Ù…Ø§ÙƒØ±ÙˆÙ† ØªØµØ±Ø­ Ø£Ù† Ø§Ù„Ø­Ø¬Ø§Ø¨ ÙŠØ±Ø¹Ø¨ ÙˆÙŠØ®ÙŠÙ Ø§Ù„Ø£Ø·ÙØ§Ù„..ðŸ˜…ðŸ˜‚ðŸ˜‚',\n",
       " 'img_path': './data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_images_part2/www.pinterest.com_pin_374924737743995066/7485ad3c9c4cd8159ce93997a18a53a8.jpg',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonl_path = \"./arabic_hateful_meme_train.jsonl\" # Example path, modify as needed\n",
    "data = []\n",
    "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    data.append(json.loads(line))\n",
    "\n",
    "# data is now a list of dictionaries, where each dictionary is a parsed JSON object from a line in the file.\n",
    "print(f\"Loaded {len(data)} entries from {jsonl_path}\")\n",
    "if data:\n",
    "    print(\"First entry:\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgkvwlbFHVo5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-De1tz5qHYre",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_model_name = 'distilbert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgNrs7AhKdvl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Loading data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LDwaW8AnKcgD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5f396886f443f895338e23d760f9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9b6e27a7ab4ecfa2dc218201762011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1e2505bf1a4329a1c4c157f14563e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badc1db63bbd4858b153a4b59dba2e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def read_jsonl_to_df(filename):\n",
    "    return pd.read_json(filename, lines=True)\n",
    "\n",
    "l2id = {'not-hate': 0, 'hate': 1}\n",
    "\n",
    "# Assume all splits use \"img_path\" as the image column\n",
    "def prepare_dataset(file):\n",
    "    df = read_jsonl_to_df(file)\n",
    "    # df['label'] = df['label'].map(l2id)\n",
    "    # Cast \"img_path\" column as Image\n",
    "    return Dataset.from_pandas(df) #.cast_column(\"img_path\", Image())\n",
    "\n",
    "train_df = prepare_dataset(train_file)\n",
    "train_dataset = MultimodalDataset(train_df['id'], train_df['text'], train_df['img_path'], train_df['label'])\n",
    "\n",
    "validation_dataset = prepare_dataset(validation_file)\n",
    "validation_dataset = MultimodalDataset(validation_dataset['id'], validation_dataset['text'], validation_dataset['img_path'], validation_dataset['label'])\n",
    "\n",
    "test_dataset = prepare_dataset(test_file)\n",
    "test_dataset = MultimodalDataset(test_dataset['id'], test_dataset['text'], test_dataset['img_path'], test_dataset['label'])\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-yDKlycyoZA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Finalize the train data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjfGaFbqdxpj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VwSFrRg4mNRh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if max_train_samples is not None:\n",
    "    max_train_samples_n = min(len(train_dataset), max_train_samples)\n",
    "    train_dataset = train_dataset.select(range(max_train_samples_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k72vUTSigOzZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Finalize the development/evaluation data for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MqrW8ospgUYZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if max_eval_samples is not None:\n",
    "    max_eval_samples_n = min(len(validation_dataset), max_eval_samples)\n",
    "    validation_dataset = validation_dataset.select(range(max_eval_samples_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7sVqp3hgU4i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Finalize the test data for predicting the unseen test data using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u0dBjIQggcYs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if max_predict_samples is not None:\n",
    "    max_predict_samples_n = min(len(test_dataset), max_predict_samples)\n",
    "    test_dataset = test_dataset.select(range(max_predict_samples_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqbo1xzRge36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Log a few random samples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIO2bxSVgkLb",
    "outputId": "997ceec2-b658-4501-8d29-b5ef5065922a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 842 of the training set: {'id': 'data/arabic_memes_fb_insta_pinterest/Pinterest/images/pinterest_images_part2/www.pinterest.com_pin_302163456262798283/596c0b3852bb183b5584759b74eec4f0.jpg', 'text': tensor([   101,  89190,  16333,  12441,  59901,  34353,  22468,    766,  11091,\n",
      "         16498,  11687,  11509,    119,    752,    763,  25683,  12616,  11722,\n",
      "         34032,    774,  10461,  59901,  23984,  35487,    106,    756,    759,\n",
      "         40926,  10388,  60844,    789, 102211,    102,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0]), 'text_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'img_path': tensor([[[ 2.2489,  2.2489,  2.2489,  ...,  1.7865,  1.8037,  1.2043],\n",
      "         [ 2.2489,  2.2489,  2.2489,  ...,  1.8550,  1.7352,  1.8550],\n",
      "         [ 2.2489,  2.2489,  2.2489,  ...,  2.0948,  2.0948,  1.8208],\n",
      "         ...,\n",
      "         [ 0.1083,  0.5536,  0.4851,  ...,  1.8550,  1.8722,  1.3413],\n",
      "         [ 0.1768,  0.5878,  0.4851,  ...,  1.6153,  1.4440,  1.7694],\n",
      "         [ 0.2624,  0.6221,  0.4851,  ...,  1.3584,  1.4954,  1.4954]],\n",
      "\n",
      "        [[ 2.4286,  2.4286,  2.4286,  ...,  0.7654,  0.4503, -0.4601],\n",
      "         [ 2.4286,  2.4286,  2.4286,  ...,  1.4657,  0.7129,  0.4853],\n",
      "         [ 2.4286,  2.4286,  2.4286,  ...,  2.3060,  1.8158,  1.2731],\n",
      "         ...,\n",
      "         [ 0.3627,  0.6954,  0.6779,  ...,  1.1155,  1.1681,  0.6429],\n",
      "         [ 0.3803,  0.6954,  0.6604,  ...,  0.7129,  0.6779,  1.2556],\n",
      "         [ 0.4503,  0.7129,  0.6429,  ...,  0.5553,  0.9405,  1.0980]],\n",
      "\n",
      "        [[ 2.6400,  2.6400,  2.6400,  ..., -0.5495, -0.8633, -1.2641],\n",
      "         [ 2.6400,  2.6400,  2.6400,  ...,  0.6705, -0.6367, -0.9330],\n",
      "         [ 2.6400,  2.6400,  2.6400,  ...,  2.0474,  1.1411,  0.2348],\n",
      "         ...,\n",
      "         [ 0.5659,  0.7228,  0.7054,  ...,  1.0888,  1.2457,  0.7751],\n",
      "         [ 0.6008,  0.7228,  0.6879,  ...,  0.6356,  0.7054,  1.3851],\n",
      "         [ 0.6705,  0.7576,  0.6879,  ...,  0.5311,  0.9668,  1.1934]]]), 'label': tensor(0)}.\n",
      "Sample 863 of the training set: {'id': 'data/arabic_memes_fb_insta_pinterest/Instagram/IMAGES/ex.officiall/2021-10-09_18-59-38_UTC.jpg', 'text': tensor([  101, 60844, 51189, 26614, 14269, 10429,   787, 76081, 22908, 31888,\n",
      "        10388, 10289, 12949, 89218, 37423,   134, 12437, 10429,   119,   119,\n",
      "          119, 59901, 40281, 44911,   118, 69765,   160, 11259, 67813, 44376,\n",
      "        12396,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'text_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'img_path': tensor([[[ 0.3652,  0.3309,  0.8789,  ..., -1.6213, -1.5014, -1.3473],\n",
      "         [ 0.3823,  0.3652,  0.9988,  ..., -1.3302, -1.4500, -1.5870],\n",
      "         [ 0.3994,  0.3994,  1.0844,  ..., -1.0562, -0.9705, -0.9877],\n",
      "         ...,\n",
      "         [ 0.6734,  0.5878,  0.5878,  ..., -1.0904, -1.0562, -1.1418],\n",
      "         [ 0.6392,  0.5878,  0.6049,  ..., -1.0562, -1.0562, -1.1418],\n",
      "         [ 0.7077,  0.6734,  0.6906,  ..., -1.0904, -1.1247, -1.2103]],\n",
      "\n",
      "        [[ 0.3978,  0.4153,  0.9755,  ..., -1.7906, -1.6681, -1.4930],\n",
      "         [ 0.3978,  0.4503,  1.1155,  ..., -1.4930, -1.5980, -1.7031],\n",
      "         [ 0.4153,  0.4678,  1.2031,  ..., -1.1954, -1.1078, -1.0553],\n",
      "         ...,\n",
      "         [-0.1625, -0.1975, -0.1800,  ..., -0.9853, -0.9503, -1.0378],\n",
      "         [-0.1975, -0.1975, -0.1625,  ..., -0.9503, -0.9503, -1.0203],\n",
      "         [-0.1275, -0.1099, -0.0749,  ..., -0.9853, -1.0203, -1.0728]],\n",
      "\n",
      "        [[ 0.3045,  0.4091,  1.0191,  ..., -1.6476, -1.5430, -1.3861],\n",
      "         [ 0.3393,  0.4439,  1.1411,  ..., -1.3861, -1.4733, -1.5953],\n",
      "         [ 0.3568,  0.4614,  1.2457,  ..., -1.1247, -1.0376, -1.0027],\n",
      "         ...,\n",
      "         [-0.0790, -0.1835, -0.1835,  ..., -0.7936, -0.7587, -0.8458],\n",
      "         [-0.1138, -0.1835, -0.1661,  ..., -0.7587, -0.7587, -0.8458],\n",
      "         [-0.0441, -0.0964, -0.0790,  ..., -0.7936, -0.8284, -0.8981]]]), 'label': tensor(0)}.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for index in random.sample(range(len(train_dataset)), 2):\n",
    "    print(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFh6wO9CuheP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tZBwbR0Aq0Fq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "validation_df = torch.utils.data.DataLoader(validation_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "test_df = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbhskCcnNYYT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MultiModal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GAkUKbHFNci_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from transformers import BertModel, AutoModel\n",
    "\n",
    "# Define the multimodal classification model\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "\n",
    "        # BERT model for text input\n",
    "        #config = AutoConfig.from_pretrained('xlm-roberta-xlarge', num_labels=2,use_auth_token=None)\n",
    "        self.bert = AutoModel.from_pretrained(text_model_name)\n",
    "\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.bert_fc = nn.Linear(768, 512) #for BERT=768\n",
    "\n",
    "        # ResNet model for image input\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet_fc = nn.Linear(1000, 512)\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion_fc = nn.Linear(1024, 512)\n",
    "        # Output layer\n",
    "        self.output_fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, text, image, mask):\n",
    "        #image = image.unsqueeze(0)\n",
    "        # Text input through BERT model\n",
    "        bert_output = self.bert(text, attention_mask=mask, return_dict=False) #attention_mask=mask,\n",
    "        #bert_output = self.bert(text, attention_mask=mask, return_dict=False) #attention_mask=mask,\n",
    "        #print(bert_output)\n",
    "        bert_output = self.bert_drop(bert_output[0][:, -1, :])\n",
    "        bert_output = self.bert_fc(bert_output)\n",
    "\n",
    "\n",
    "        # Image input through ResNet model\n",
    "        resnet_output = self.resnet(image)\n",
    "        resnet_output = self.resnet_fc(resnet_output)\n",
    "\n",
    "        # Concatenate the text and image features\n",
    "        # bert_output = bert_output.squeeze(2)\n",
    "        # print(bert_output.shape)\n",
    "        # print(resnet_output.shape)\n",
    "        features = torch.cat((bert_output, resnet_output), dim=1)\n",
    "\n",
    "        # Fusion layer\n",
    "        features = self.fusion_fc(features)\n",
    "        # Output layer\n",
    "        output = self.output_fc(features)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Define the training and testing functions\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    for data in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        text = data[\"text\"].to(device)\n",
    "        #print(text.shape)\n",
    "        image = data[\"img_path\"].to(device)\n",
    "        mask = data[\"text_mask\"].to(device)\n",
    "        #print(mask.shape)\n",
    "        labels = data['label'].to(device)\n",
    "        output = model(text, image, mask)\n",
    "        #print(output)\n",
    "        loss = criterion(output, labels)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            text = data[\"text\"].to(device)\n",
    "            image = data[\"img_path\"].to(device)\n",
    "            mask = data[\"text_mask\"].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            output = model(text, image, mask)\n",
    "            loss = criterion(output, labels)\n",
    "            test_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afPkDvllPsnC",
    "outputId": "43c12bcd-4cbf-48bf-8994-c24821cb2c8a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e3267f2cc743088722e46253812c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/studenten-temp1/users/yassir/yass_env/lib64/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mount/studenten-temp1/users/yassir/yass_env/lib64/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /mount/studenten-temp1/users/yassir/yass_env_cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 278MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 267/267 [01:17<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: Train Loss = 0.3273, Accuracy = 0.8955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultimodalClassifier(num_classes=2)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, acc = train(model, train_df, criterion, optimizer, device)\n",
    "    #dev_loss, accuracy = test(model, eval_dataset, criterion, device)\n",
    "    print('Epoch {}/{}: Train Loss = {:.4f}, Accuracy = {:.4f}'.format(epoch+1, num_epochs, train_loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-teCIiNavdSJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8S3ctqRuvbQ-",
    "outputId": "b1ca709a-8f2c-4aef-bbc4-3d709bd12772",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:02<00:00, 14.24it/s]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    y_test_pred = []\n",
    "    ids = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            text = data[\"text\"].to(device)\n",
    "            image = data[\"img_path\"].to(device)\n",
    "            mask = data[\"text_mask\"].to(device)\n",
    "            output = model(text, image, mask)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predictions.append(predicted)\n",
    "            ids.append(data[\"id\"])\n",
    "\n",
    "    with open(f'task2_TeamName.csv', 'w') as f:\n",
    "      f.write(\"id\\tprediction\\n\")\n",
    "      indx = 0\n",
    "      id2l = {0:'not-hate', 1:'hate'}\n",
    "      for i, line in enumerate(predictions):\n",
    "        for indx, l in enumerate(line.tolist()):\n",
    "          f.write(f\"{ids[i][indx]}\\t{id2l[l]}\\n\")\n",
    "\n",
    "evaluate(model, validation_df, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbSWop7viib9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "yass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
